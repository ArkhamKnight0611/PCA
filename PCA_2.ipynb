{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Projections and PCA\n",
        "\n",
        "Projection: In linear algebra, a projection takes a data point in a higher-dimensional space and maps it onto a lower-dimensional subspace. Imagine projecting a 3D object onto a flat surface â€“ that's the essence of projection.\n",
        "PCA and Projections: In PCA, projections are used to represent the data points on the principal components (PCs). These PCs are new axes created by PCA that capture the most significant variance in the data. By projecting the data points onto these PCs, we effectively reduce the dimensionality while retaining the most important information.\n",
        "\n",
        "Q2. Optimization in PCA\n",
        "\n",
        "The Problem: PCA aims to find a set of orthogonal directions (principal components) in the high-dimensional space that maximizes the variance captured when the data points are projected onto them. Think of it as finding the \"best\" directions to stretch the data along while keeping most of the spread.\n",
        "What it Achieves: This optimization helps PCA achieve dimensionality reduction while preserving the information that contributes most to the data's variability. By focusing on directions with high variance, PCA captures the underlying structure of the data efficiently.\n",
        "\n",
        "Q3. Covariance Matrices and PCA\n",
        "\n",
        "Covariance Matrix: The covariance matrix captures the linear relationships between features in the data. It shows how much two features tend to vary together.\n",
        "PCA and Covariance: PCA uses the covariance matrix to determine the directions of the principal components. The eigenvectors of the covariance matrix correspond to the principal components, and the eigenvalues represent the variance captured by each component.\n",
        "\n",
        "Q4. Choosing the Number of Components\n",
        "\n",
        "Impact on Performance: The number of principal components chosen affects the trade-off between dimensionality reduction and information loss.\n",
        "Considerations: Choosing too few components might discard important information, while choosing too many might retain noise. Common approaches include using explained variance ratio (percentage of variance captured) or techniques like the elbow method to identify an optimal stopping point.\n",
        "\n",
        "Q5. PCA for Feature Selection\n",
        "\n",
        "Selection via PCA: By analyzing the variance captured by each principal component, we can identify features that contribute more to the overall variation. Features associated with high variance components might be more relevant.\n",
        "Benefits: PCA helps select features in an unsupervised way, without relying on labels. It can also remove redundant features that capture similar information.\n",
        "\n",
        "Q6. Applications of PCA\n",
        "\n",
        "Data Visualization: PCA is often used for dimensionality reduction before visualizing high-dimensional data in lower dimensions (e.g., from 3D to 2D for scatter plots).\n",
        "Anomaly Detection: By identifying data points that lie far from the principal components, PCA can help detect anomalies or outliers in the data.\n",
        "Machine Learning Preprocessing: PCA can be used as a preprocessing step in various machine learning algorithms to improve their performance by reducing dimensionality and potentially mitigating the curse of dimensionality.\n",
        "\n",
        "Q7. Spread and Variance in PCA\n",
        "\n",
        "Spread and Variance: Spread refers to how dispersed the data points are around the mean, while variance is the squared spread, a measure of how much the data deviates from the average.\n",
        "PCA and Spread/Variance: PCA focuses on capturing the directions of greatest variance. By maximizing variance in the projected data, PCA effectively captures the spread of the data along the most informative directions.\n",
        "\n",
        "Q8. Identifying Principal Components\n",
        "\n",
        "PCA leverages variance: PCA analyzes the covariance matrix to identify eigenvectors with the highest eigenvalues. These eigenvectors correspond to the principal components, representing directions of high variance in the data.\n",
        "\n",
        "Q9. PCA and Uneven Variance\n",
        "\n",
        "Handling Uneven Variance: PCA inherently prioritizes components with higher variance. This means features with high variance will have a greater influence on the principal components.\n",
        "Normalization: To address this, data can be normalized before applying PCA. Normalization scales features to have a similar range, ensuring features with lower inherent variance have a fairer chance of contributing to the principal components."
      ],
      "metadata": {
        "id": "gBpPp0q_9ZYH"
      }
    }
  ]
}